# ‚úàÔ∏è AkashInsights: Dual-Agent Aerospace AI System

**Aircraft Health Intelligence Platform** combining **Machine Ear** (acoustic fault detection) + **Human Ear** (speech recognition & stress analysis) for comprehensive aerospace health monitoring.

---

## üéØ Project Overview

AkashInsights is an end-to-end AI system that:
- **Predicts engine faults** through acoustic analysis (Machine Ear)
- **Monitors crew stress** via speech recognition (Human Ear)
- **Fuses insights** for unified health scoring
- **Provides real-time dashboard** for monitoring and decision-making
- **Supports multilingual** communication (Make-in-India initiative)

---

## üìä Week 2 Achievements

- ‚úÖ Completed project ideation & architecture planning
- ‚úÖ Created GitHub repo: [AkashInsights](https://github.com/DakshMehta29/AkashInsights)
- ‚úÖ Downloaded datasets (CMAPSS + acoustic + speech)
- ‚úÖ Set up project folder structure
- ‚úÖ Preprocessed dataset samples
- ‚úÖ Built initial Machine Ear MFCC pipeline
- ‚úÖ Added baseline acoustic anomaly detection model
- ‚úÖ Integrated Whisper for speech-to-text
- ‚úÖ Created initial dual-agent Streamlit prototype
- ‚úÖ Trained RandomForest baseline (MAE: 11.05 cycles, R¬≤: 0.942)

---

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AkashInsights Platform                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  Machine Ear     ‚îÇ         ‚îÇ  Human Ear       ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  (Agent 1)      ‚îÇ         ‚îÇ  (Agent 2)      ‚îÇ         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Acoustic CNN   ‚îÇ         ‚îÇ ‚Ä¢ Whisper STT    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Fault Detection‚îÇ         ‚îÇ ‚Ä¢ Stress Analysis‚îÇ         ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ RUL Prediction ‚îÇ         ‚îÇ ‚Ä¢ Translation    ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ           ‚îÇ                             ‚îÇ                   ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                      ‚îÇ                                      ‚îÇ
‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ           ‚îÇ Composite Engine    ‚îÇ                          ‚îÇ
‚îÇ           ‚îÇ (Fusion Agent)      ‚îÇ                          ‚îÇ
‚îÇ           ‚îÇ ‚Ä¢ Weighted Scoring  ‚îÇ                          ‚îÇ
‚îÇ           ‚îÇ ‚Ä¢ Status: Safe/Caution/Critical                ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îÇ                      ‚îÇ                                      ‚îÇ
‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ           ‚îÇ Streamlit Dashboard  ‚îÇ                          ‚îÇ
‚îÇ           ‚îÇ ‚Ä¢ Real-time Monitor ‚îÇ                          ‚îÇ
‚îÇ           ‚îÇ ‚Ä¢ Analytics         ‚îÇ                          ‚îÇ
‚îÇ           ‚îÇ ‚Ä¢ Blockchain Log    ‚îÇ                          ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÇ Repository Structure

```
AkashInsights/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ CMaps/                    # NASA CMAPSS raw data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_FD001.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_FD001.txt
‚îÇ   ‚îú‚îÄ‚îÄ acoustic/                 # Acoustic training data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normal/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fault1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fault2/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fault3/
‚îÇ   ‚îú‚îÄ‚îÄ speech/                   # Speech samples
‚îÇ   ‚îú‚îÄ‚îÄ train_cleaned.csv
‚îÇ   ‚îú‚îÄ‚îÄ test_cleaned.csv
‚îÇ   ‚îî‚îÄ‚îÄ processed/               # Preprocessed arrays
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ acoustic_preprocessing.py # MFCC, Mel Spec, FFT extraction
‚îÇ   ‚îú‚îÄ‚îÄ acoustic_model.py         # CNN/CRNN training
‚îÇ   ‚îú‚îÄ‚îÄ acoustic_inference.py     # Real-time prediction
‚îÇ   ‚îú‚îÄ‚îÄ speech_agent.py           # Whisper + stress detection
‚îÇ   ‚îú‚îÄ‚îÄ translator.py             # Multilingual (IndicTrans)
‚îÇ   ‚îú‚îÄ‚îÄ composite_engine.py      # Fusion agent
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py              # Blockchain log + emissions
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                  # Helper functions
‚îÇ
‚îú‚îÄ‚îÄ scripts/                      # Week 2 baseline scripts
‚îÇ   ‚îú‚îÄ‚îÄ load_data.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py
‚îÇ   ‚îú‚îÄ‚îÄ train_rf_model.py
‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_load_and_clean.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 02_train_model.ipynb
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ rf_model.pkl              # Week 2 baseline
‚îÇ   ‚îî‚îÄ‚îÄ acoustic_model.h5         # Week 3 CNN model
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_report.txt
‚îÇ   ‚îî‚îÄ‚îÄ feature_importance.csv
‚îÇ
‚îú‚îÄ‚îÄ streamlit_app.py              # Main dashboard
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/DakshMehta29/AkashInsights.git
cd AkashInsights

# Install dependencies
pip install -r requirements.txt

# Optional: Install Whisper (if not included in requirements)
pip install openai-whisper

# Optional: Install IndicTrans for better Indian language support
pip install indicTrans
```

### 2. Prepare Data

Place your datasets in the appropriate directories:

- **CMAPSS data**: `data/CMaps/train_FD001.txt`, `test_FD001.txt`
- **Acoustic data**: `data/acoustic/{normal,fault1,fault2,fault3}/*.wav`
- **Speech samples**: `data/speech/*.wav` (optional)

### 3. Train Models

#### Week 2 Baseline (RUL Prediction):
```bash
# Load and clean data
python scripts/load_data.py

# Preprocess
python scripts/preprocess.py

# Train RandomForest
python scripts/train_rf_model.py
```

#### Week 3 Acoustic Model:
```bash
# Train CNN/CRNN for fault detection
python -c "from src.acoustic_model import train_model; from pathlib import Path; train_model(Path('data/acoustic'), model_type='cnn', epochs=50, model_save_path=Path('models/acoustic_model.h5'))"
```

### 4. Run Dashboard

```bash
streamlit run streamlit_app.py
```

Access at: `http://localhost:8501`

---

## üß† Core Components

### 1. Machine Ear (Acoustic Agent)

**File**: `src/acoustic_preprocessing.py`, `src/acoustic_model.py`, `src/acoustic_inference.py`

**Features**:
- MFCC, Mel Spectrogram, FFT feature extraction
- Data augmentation (time-stretch, noise, pitch shift, gain)
- CNN/CRNN models for fault classification
- Real-time inference from audio files or microphone

**Usage**:
```python
from src.acoustic_inference import predict_audio, predict_from_mic

# Predict from file
result = predict_audio("data/acoustic/test.wav")
print(f"Class: {result['predicted_class']}, Confidence: {result['confidence']}")

# Predict from microphone
result = predict_from_mic(duration=3.0)
```

**Classes**: Normal, Fault1, Fault2, Fault3

**Target Accuracy**: >90%

---

### 2. Human Ear (Speech Agent)

**File**: `src/speech_agent.py`

**Features**:
- Whisper-based speech-to-text
- Stress detection (RMS energy, pitch variation, voice tremor, MFCC delta)
- Real-time transcription from microphone

**Usage**:
```python
from src.speech_agent import SpeechAgent

agent = SpeechAgent(model_name="base")
result = agent.analyze_speech(audio_path="speech.wav")
print(f"Transcription: {result['transcription']}")
print(f"Stress Level: {result['stress_level']}")
```

**Output**:
```json
{
  "transcription": "...",
  "stress_level": "low/medium/high",
  "stress_score": 0.35,
  "confidence": 0.94
}
```

---

### 3. Translation Module (Make-in-India)

**File**: `src/translator.py`

**Supported Languages**: Hindi, Tamil, Bengali, Telugu, Marathi, Gujarati, Kannada, Malayalam, Punjabi, Urdu

**Usage**:
```python
from src.translator import Translator

translator = Translator()
result = translator.translate_text("Engine status is normal", "hindi")
print(result["translated"])
```

**Backends** (priority order):
1. IndicTrans (best for Indian languages)
2. IndicBERT (transformers-based)
3. Google Translate (fallback)

---

### 4. Composite Health Engine

**File**: `src/composite_engine.py`

**Fusion Formula**:
```
composite_score = 0.6 * machine_score + 0.4 * human_stress_index
```

**Status Levels**:
- **Safe**: composite_score ‚â• 0.7
- **Caution**: 0.4 ‚â§ composite_score < 0.7
- **Critical**: composite_score < 0.4

**Usage**:
```python
from src.composite_engine import CompositeHealthEngine

engine = CompositeHealthEngine()
result = engine.analyze_complete(
    audio_path="engine.wav",
    speech_stress={"stress_level": "low", "stress_score": 0.2}
)
print(f"System Status: {result['system_status']}")
```

---

### 5. Streamlit Dashboard

**File**: `streamlit_app.py`

**Features**:
- **Machine Health Monitor**: Upload audio, live recording, spectrograms, fault prediction
- **Crew Communication**: Live transcription, stress analysis, translation
- **Analytics**: Maintenance history, composite score trends, blockchain verification
- **Emission Optimization**: Fuel savings recommendations based on engine health
- **Voice Commands**: "Show engine status", "Translate message", etc.

**Tabs**:
1. üè† Dashboard - System overview, status banner, quick stats
2. üîä Machine Health - Audio upload, live recording, predictions
3. üë• Crew Communication - Speech transcription, stress detection, translation
4. üìä Analytics - Historical data, charts, blockchain log
5. ‚öôÔ∏è Settings - Model configuration, system info

---

## üîê Advanced Features

### Blockchain-like Maintenance Log

**File**: `src/dashboard.py` ‚Üí `MaintenanceLog` class

- SHA256 hash chain for each record
- Timestamp, fault prediction, stress level, composite score
- Chain integrity verification
- SQLite database storage

**Usage**:
```python
from src.dashboard import MaintenanceLog

log = MaintenanceLog()
hash_val = log.add_record(
    machine_status="normal",
    fault_prediction="none",
    stress_level="low",
    composite_score=0.85,
    system_status="safe"
)
is_valid = log.verify_chain()  # True
```

---

### Emission Reduction Agent

**File**: `src/dashboard.py` ‚Üí `EmissionsAgent` class

- Recommends optimal altitude and throttle based on engine health
- Estimates fuel savings (3-8%) and CO‚ÇÇ reduction
- Mock simulation for demonstration

**Usage**:
```python
from src.dashboard import EmissionsAgent

recommendations = EmissionsAgent.recommend_optimization(
    anomaly_prob=0.15,
    current_altitude=35000
)
print(f"Fuel Savings: {recommendations['fuel_savings_pct']}%")
```

---

## üìà Model Performance

### Week 2 Baseline (RandomForest)
- **MAE**: 11.05 cycles ‚úÖ (Goal: <20)
- **RMSE**: 16.28 cycles
- **R¬≤**: 0.942

### Week 3 Acoustic Model (Target)
- **Accuracy**: >90% (fault classification)
- **Classes**: Normal, Fault1, Fault2, Fault3

---

## üõ†Ô∏è Development

### Running Tests

```bash
# Test acoustic preprocessing
python -c "from src.acoustic_preprocessing import extract_all_features; import numpy as np; features = extract_all_features(np.random.randn(22050)); print('‚úÖ Preprocessing works')"

# Test speech agent
python src/speech_agent.py

# Test composite engine
python src/composite_engine.py
```

### Code Style

- PEP8 compliant
- Type hints where applicable
- Docstrings for all functions
- Modular design

---

## üìù Dataset Sources

1. **NASA CMAPSS**: Turbofan Engine Degradation Simulation
   - Source: https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/
   - Files: `train_FD001.txt`, `test_FD001.txt`

2. **Acoustic Data**: Engine sound samples (user-provided)
   - Structure: `data/acoustic/{normal,fault1,fault2,fault3}/*.wav`

3. **Speech Data**: Crew communication samples (user-provided)
   - Location: `data/speech/*.wav`

---

## üéØ Future Enhancements

- [ ] Real-time streaming audio analysis
- [ ] Advanced fusion architectures (attention-based)
- [ ] Multi-engine fleet monitoring
- [ ] Mobile app integration
- [ ] Cloud deployment (AWS/Azure)
- [ ] Edge device optimization (TensorFlow Lite)

---

## ü§ù Contributing

PRs welcome! Please:
- Follow PEP8 style guide
- Add docstrings to new functions
- Include tests for new features
- Update README if adding major features

---

## üìÑ License

This project is part of an academic/research initiative. Please cite appropriately if used in research.

---

## üë®‚Äçüíª Author

**Daksh Mehta**
- GitHub: [@DakshMehta29](https://github.com/DakshMehta29)
- Repository: [AkashInsights](https://github.com/DakshMehta29/AkashInsights)

---

## üôè Acknowledgments

- NASA for CMAPSS dataset
- OpenAI for Whisper model
- Librosa team for audio processing
- Streamlit for dashboard framework
- IndicTrans for multilingual support

---

**Built with ‚ù§Ô∏è for Aerospace AI Innovation**
